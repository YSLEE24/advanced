# build_faiss_vectorstore.py

from bs4 import BeautifulSoup
from pathlib import Path
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
import os
import ast

def extract_and_save_faiss(base_dir="templates/contents", out_path="faiss_store/contents"):
    contentList = []  # ë¬¸ì„œ ë‚´ì˜ ëª¨ë“  chunkë¥¼ ëª¨ì•„ ë‘˜ ë¦¬ìŠ¤íŠ¸

    # âœ… í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •: chunkëŠ” 600ì, 80ì ê²¹ì¹˜ê¸° (ì¤‘ë³µ ë¬¸ë§¥ ë³´ì¡´)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=80,
        separators=["\n\n", "\n", ".", " "],
    )

    # âœ… templates/contents í•˜ìœ„ì˜ ëª¨ë“  HTML íŒŒì¼ íƒìƒ‰
    for file_path in Path(base_dir).rglob("*.html"):
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()

        # ë©”íƒ€ë°ì´í„°: í´ë”ëª…ê³¼ íŒŒì¼ëª… ì¶”ì¶œ
        folder = os.path.basename(os.path.dirname(file_path))
        page = os.path.splitext(os.path.basename(file_path))[0]

        # âœ… í…œí”Œë¦¿ íƒ­ êµ¬ì¡° ì²˜ë¦¬: {% set tabs = [ ... ] %} í˜•íƒœì˜ íƒ­ì´ ìˆëŠ” ê²½ìš°
        import re

        if '{% set tabs = [' in html_content:
            try:
                # âœ… ì •ê·œì‹ìœ¼ë¡œ ëª¨ë“  íƒ­ ë¸”ë¡ ì¶”ì¶œ
                tabs_raw = re.findall(r"\(\s*'([^']+)'\s*,\s*'([^']+)'\s*,\s*'''(.*?)'''\s*\)", html_content, re.DOTALL)
                
                for tab_id, tab_title, tab_html in tabs_raw:
                    section_soup = BeautifulSoup(tab_html, 'html.parser')
                    section_text = section_soup.get_text(separator="\n", strip=True)
                    chunks = splitter.split_text(section_text)
                    for chunk in chunks:
                        contentList.append([tab_title, chunk, folder, page])

            except Exception as e:
                print(f"âš ï¸ íƒ­ íŒŒì‹± ì‹¤íŒ¨ (ì •ê·œì‹ ë²„ì „): {file_path}, {e}")


        else:
            # âœ… ì¼ë°˜ HTML êµ¬ì¡° (<div name="section">) ì²˜ë¦¬
            soup = BeautifulSoup(html_content, 'html.parser')
            sections = soup.select('[name="section"]')  # ì½˜í…ì¸  ì„¹ì…˜ ì¶”ì¶œ

            for section in sections:
                titles = section.select('[name="content_title"]')
                title = titles[0].get_text(strip=True) if len(titles) > 0 else ""

                section_text = section.get_text(separator="\n", strip=True)
                chunks = splitter.split_text(section_text)

                for chunk in chunks:
                    contentList.append([title, chunk, folder, page])

    # âœ… ë°ì´í„°í”„ë ˆì„ ë³€í™˜ (í™•ì¸ ë° ì •ì œ ëª©ì )
    df = pd.DataFrame(contentList, columns=["title", "content", "folder", "page"])

    # âœ… LangChain ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜ (page_content + ë©”íƒ€ë°ì´í„°)
    documents = [
        Document(
            page_content=f"{row['title']}\n{row['page']}\n{row['content']}",
            metadata={
                "title": row['title'],
                "folder": row['folder'],
                "page": row['page'],
                "source": f"templates/contents/{row['folder']}/{row['page']}.html"
            }
        )
        for _, row in df.iterrows()
    ]

    # âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    # embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta")
    # embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    # embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    # embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
    embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta-multitask")

    # âœ… í…ŒìŠ¤íŠ¸ ì¶œë ¥: 'ê·€ì–´' í‚¤ì›Œë“œê°€ í¬í•¨ëœ chunk ë¡œê·¸ í™•ì¸
    print("\nğŸ“Œ 'ê·€ì–´'ê°€ í¬í•¨ëœ chunkë“¤:")
    for i, doc in enumerate(documents):
        if "ê·€ì–´" in doc.page_content:
            print(f"\n[{i}] {doc.metadata.get('source')}")
            print(doc.page_content[:300])

    # âœ… FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥
    vectorstore = FAISS.from_documents(documents=documents, embedding=embedding_model)
    vectorstore.save_local(out_path)
    print(f"ì €ì¥ ì™„ë£Œ: {len(documents)} chunks â†’ {out_path}")

# âœ… ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ í•¨ìˆ˜ í˜¸ì¶œ
if __name__ == "__main__":
    extract_and_save_faiss()




# # build_faiss_vectorstore.py
# from bs4 import BeautifulSoup
# from pathlib import Path
# import pandas as pd
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.vectorstores import FAISS
# from langchain.schema import Document
# import os
# import ast

# def extract_and_save_faiss(base_dir="templates/contents", out_path="faiss_store/contents"):
#     contentList = [] # ë¬¸ì„œë“¤ì˜(contents ë‚´ì˜) ëª¨ë“  chunkë¥¼ ì„ì‹œë¡œ ëª¨ìœ¼ê¸°

#     # í…ìŠ¤íŠ¸ ë¶„í• 
#     splitter = RecursiveCharacterTextSplitter(
#         chunk_size=600,
#         chunk_overlap=80,
#         separators=["\n\n", "\n", ".", " "],
#     )

#     # templates/contents í•˜ìœ„ì˜ ëª¨ë“  html íŒŒì¼ íƒìƒ‰
#     for file_path in Path(base_dir).rglob("*.html"):
#         with open(file_path, 'r', encoding='utf-8') as f:
#             html_content = f.read()

#         # ê²½ë¡œì—ì„œ í´ë”ëª…ê³¼ íŒŒì¼ëª…ì„ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ìš©)
#         folder = os.path.basename(os.path.dirname(file_path))
#         page = os.path.splitext(os.path.basename(file_path))[0]

#     for file_path in Path(base_dir).rglob("*.html"):
#         with open(file_path, 'r', encoding='utf-8') as f:
#             html_content = f.read()

#         folder = os.path.basename(os.path.dirname(file_path))
#         page = os.path.splitext(os.path.basename(file_path))[0]

#         if '{% set tabs = [' in html_content:
#             tab_start = html_content.index('{% set tabs = [')
#             tab_end = html_content.index('%}', tab_start)
#             tab_block = html_content[tab_start + 16:tab_end]

#             try:
#                 tab_list = ast.literal_eval(tab_block.replace("'", '"'))
#                 for _, _, raw_html in tab_list:
#                     section_soup = BeautifulSoup(raw_html, 'html.parser')
#                     section_text = section_soup.get_text(separator="\n", strip=True)
#                     chunks = splitter.split_text(section_text)
#                     for chunk in chunks:
#                         contentList.append(["ìŠ¤ë§ˆíŠ¸ì–‘ì‹", chunk, folder, page])
#             except Exception as e:
#                 print(f"âš ï¸ íƒ­ íŒŒì‹± ì‹¤íŒ¨: {file_path}, {e}")
#         else:
#             soup = BeautifulSoup(html_content, 'html.parser')
#             sections = soup.select('[name="section"]')
#             for section in sections:
#                 titles = section.select('[name="content_title"]')
#                 title = titles[0].get_text(strip=True) if len(titles) > 0 else ""
#                 section_text = section.get_text(separator="\n", strip=True)
#                 chunks = splitter.split_text(section_text)
#                 for chunk in chunks:
#                     contentList.append([title, chunk, folder, page])

#         # # HTML íŒŒì‹±
#         # soup = BeautifulSoup(html_content, 'html.parser')

#         # # # div name="section" íƒœê·¸ë“¤ ì°¾ê¸° (conten_titleì´ë‘ content_textë¡œ ë‚˜ëˆ ë†¨ìŒ)
#         # sections = soup.select('[name="section"]')
#         # for section in sections:
#         #     titles = section.select('[name="content_title"]')
#         #     # contents = section.select('[name="content_text"]')
#         #     # ì œëª©ê³¼ ë³¸ë¬¸ì´ ì •í™•íˆ í•˜ë‚˜ì”© ìˆì„ ë•Œë§Œ ì²˜ë¦¬ (1:1ë¡œ ì„¤ì •, ì •í™•íˆ í•œ ìŒì¼ ë•Œë§Œ ì²˜ë¦¬)
#         #     # if len(titles) == 1 and len(contents) == 1:
#         #     #     title = titles[0].get_text(strip=True) # stripìœ¼ë¡œ íƒœê·¸ ì œê±°, ê³µë°± ì œê±°
#         #     #     content = contents[0].get_text(strip=True)
#         #     #     # ë³¸ë¬¸ì„ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° -> ì •ë°€ë„ ë†’ì´ê¸° ìœ„í•¨(ë„ˆë¬´ ê¸¸ë©´ ì •í™•ë„ ë–¨ì–´ì§..)
#         #     #     chunks = splitter.split_text(content)
#         #     #     for chunk in chunks:
#         #     #         # ê° ì¡°ê°ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (title, chunk ë‚´ìš©, í´ë”, íŒŒì¼ëª…)
#         #     #         #contentList.append([title, chunk, folder, page])
#         #     #         contentList.append([title, f"{title}\n{chunk}", folder, page])
            
#         #     # title ë¨¼ì € ì¶”ì¶œ (ìˆì„ ë•Œë§Œ)
#         #     title = titles[0].get_text(strip=True) if len(titles) > 0 else ""

#         #     # ì „ì²´ section í…ìŠ¤íŠ¸ ì¶”ì¶œ (title, ë³¸ë¬¸ ë“± ì „ë¶€)
#         #     section_text = section.get_text(separator="\n", strip=True)

#         #     # ì „ì²´ë¥¼ í•©ì³ì„œ ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ìƒì„±ì— í™œìš©
#         #     # full_text = f"{title}\n{page}\n{section_text}"
#         #     if title and title not in section_text:
#         #         full_text = f"{title}\n{section_text}\n{page}"
#         #     else:
#         #         full_text = f"{section_text}\n{page}"

#         #     chunks = splitter.split_text(full_text)
#         #     for chunk in chunks:
#         #         contentList.append([title, chunk, folder, page])

#     # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
#     df = pd.DataFrame(contentList, columns=["title", "content", "folder", "page"])

#     # LangChain Document ê°ì²´ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° í¬í•¨)
#     documents = [
#         Document(
#             page_content=f"{row['title']}\n{row['page']}\n{row['content']}",
#             metadata={
#                 "title": row['title'],
#                 "folder": row['folder'],
#                 "page": row['page'],
#                 "source": f"templates/contents/{row['folder']}/{row['page']}.html"
#             }
#         )
#         for _, row in df.iterrows()
#     ]


#     # ì„ë² ë”© ëª¨ë¸ LOAD
#     # embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
#     embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta")
#     # embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
#     # embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
#     # embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta-multitask")


#     # í…ŒìŠ¤íŠ¸ìš©
#     print("\nğŸ“Œ 'ê·€ì–´'ê°€ í¬í•¨ëœ chunkë“¤:")
#     for i, doc in enumerate(documents):
#         if "ê·€ì–´" in doc.page_content:
#             print(f"\n[{i}] {doc.metadata.get('source')}")
#             print(doc.page_content[:300])


#     # FAISS ë²¡í„° DB ìƒì„±
#     vectorstore = FAISS.from_documents(documents=documents, embedding=embedding_model)
#     vectorstore.save_local(out_path) # ì§€ì • ê²½ë¡œì— ì €ì¥
#     print(f"ì €ì¥ ì™„ë£Œ: {len(documents)} chunks â†’ {out_path}") # ì™„ë£Œ ë¡œê·¸

# # ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ í˜¸ì¶œ
# if __name__ == "__main__":
#     extract_and_save_faiss()
