# build_faiss_vectorstore.py
from bs4 import BeautifulSoup
from pathlib import Path
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
import os

def extract_and_save_faiss(base_dir="templates/contents", out_path="faiss_store/contents"):
    contentList = [] # ë¬¸ì„œë“¤ì˜(contents ë‚´ì˜) ëª¨ë“  chunkë¥¼ ì„ì‹œë¡œ ëª¨ìœ¼ê¸°

    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=40,
        separators=["\n\n", "\n", ".", " ", ""],
    )

    # templates/contents í•˜ìœ„ì˜ ëª¨ë“  html íŒŒì¼ íƒìƒ‰
    for file_path in Path(base_dir).rglob("*.html"):
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()

        # ê²½ë¡œì—ì„œ í´ë”ëª…ê³¼ íŒŒì¼ëª…ì„ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ìš©)
        folder = os.path.basename(os.path.dirname(file_path))
        page = os.path.splitext(os.path.basename(file_path))[0]

        # HTML íŒŒì‹±
        soup = BeautifulSoup(html_content, 'html.parser')

        # # div name="section" íƒœê·¸ë“¤ ì°¾ê¸° (conten_titleì´ë‘ content_textë¡œ ë‚˜ëˆ ë†¨ìŒ)
        sections = soup.select('[name="section"]')
        for section in sections:
            titles = section.select('[name="content_title"]')
            # contents = section.select('[name="content_text"]')
            # ì œëª©ê³¼ ë³¸ë¬¸ì´ ì •í™•íˆ í•˜ë‚˜ì”© ìˆì„ ë•Œë§Œ ì²˜ë¦¬ (1:1ë¡œ ì„¤ì •, ì •í™•íˆ í•œ ìŒì¼ ë•Œë§Œ ì²˜ë¦¬)
            # if len(titles) == 1 and len(contents) == 1:
            #     title = titles[0].get_text(strip=True) # stripìœ¼ë¡œ íƒœê·¸ ì œê±°, ê³µë°± ì œê±°
            #     content = contents[0].get_text(strip=True)
            #     # ë³¸ë¬¸ì„ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° -> ì •ë°€ë„ ë†’ì´ê¸° ìœ„í•¨(ë„ˆë¬´ ê¸¸ë©´ ì •í™•ë„ ë–¨ì–´ì§..)
            #     chunks = splitter.split_text(content)
            #     for chunk in chunks:
            #         # ê° ì¡°ê°ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (title, chunk ë‚´ìš©, í´ë”, íŒŒì¼ëª…)
            #         #contentList.append([title, chunk, folder, page])
            #         contentList.append([title, f"{title}\n{chunk}", folder, page])
            
            # title ë¨¼ì € ì¶”ì¶œ (ìˆì„ ë•Œë§Œ)
            title = titles[0].get_text(strip=True) if len(titles) > 0 else ""

            # ì „ì²´ section í…ìŠ¤íŠ¸ ì¶”ì¶œ (title, ë³¸ë¬¸ ë“± ì „ë¶€)
            section_text = section.get_text(separator="\n", strip=True)

            # ì „ì²´ë¥¼ í•©ì³ì„œ ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ìƒì„±ì— í™œìš©
            # full_text = f"{title}\n{page}\n{section_text}"
            if title:
                full_text = f"{title}\n{section_text}\n{page}"
            else:
                full_text = f"{section_text}\n{page}"

            chunks = splitter.split_text(full_text)
            for chunk in chunks:
                contentList.append([title, chunk, folder, page])

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(contentList, columns=["title", "content", "folder", "page"])

    # LangChain Document ê°ì²´ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° í¬í•¨)
    documents = [
        Document(
            page_content=f"{row['title']}\n{row['page']}\n{row['content']}",
            metadata={
                "title": row['title'],
                "folder": row['folder'],
                "page": row['page'],
                "source": f"templates/contents/{row['folder']}/{row['page']}.html"
            }
        )
        for _, row in df.iterrows()
    ]


    # ì„ë² ë”© ëª¨ë¸ LOAD
    # embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    # embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta")
    embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    # embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
    # embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta-multitask")


    print("\nğŸ“Œ 'ê·€ì–´'ê°€ í¬í•¨ëœ chunkë“¤:")
    for i, doc in enumerate(documents):
        if "ê·€ì–´" in doc.page_content:
            print(f"\n[{i}] {doc.metadata.get('source')}")
            print(doc.page_content[:300])


    # FAISS ë²¡í„° DB ìƒì„±
    vectorstore = FAISS.from_documents(documents=documents, embedding=embedding_model)
    vectorstore.save_local(out_path) # ì§€ì • ê²½ë¡œì— ì €ì¥
    print(f"ì €ì¥ ì™„ë£Œ: {len(documents)} chunks â†’ {out_path}") # ì™„ë£Œ ë¡œê·¸

# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ í˜¸ì¶œ
if __name__ == "__main__":
    extract_and_save_faiss()
